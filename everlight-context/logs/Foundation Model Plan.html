<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Foundation Model Plan</title>
</head>
<body>
<hr />
<p>conversation_id: "689f33c9-0cf0-8329-ab0b-0ee2177f3e08"
title: "Foundation Model Plan"
accounts: ["Account1_exports"]
models: []
tags: []
message_count: 21</p>
<hr />
<h1 id="foundation-model-plan">Foundation Model Plan</h1>
<h2 id="system">— system</h2>
<h2 id="2025-08-15t131905530577z-system">2025-08-15T13:19:05.530577Z — system</h2>
<h2 id="2025-08-15t131906030280z-tool">2025-08-15T13:19:06.030280Z — tool</h2>
<h2 id="2025-08-15t131906721000z-user">2025-08-15T13:19:06.721000Z — user</h2>
<p>EverLight OS — Foundation Model Plan (Voyagers + EnergeticSynthesis)
0) Green-lights &amp; guardrails</p>
<p>Rights/IP: Get written permission to train on EnergeticSynthesis.com content (copyright).</p>
<p>Privacy: Scrub PII; log removals.</p>
<p>Attribution mode: Keep doc IDs to enable citations.</p>
<p>1) Data fabric (three-key set)</p>
<p>Corpora</p>
<p>V1: Voyagers Material — part 1</p>
<p>V2: Voyagers Material — part 2</p>
<p>ES: EnergeticSynthesis.com (all articles/posts)</p>
<p>Ingestion → Lake</p>
<p>Crawl/collect → S3 (s3://everlight/raw/{V1|V2|ES}/...)</p>
<p>Normalize to JSONL:
{"doc_id","source":{"set":"V1|V2|ES"}, "title","author","date","url","section","text"}</p>
<p>Chunking: 800–1200 tokens with 10–15% overlap; keep paragraph &amp; section boundaries.</p>
<p>Metadata: topic tags (mythic, esoteric, technique, protocol), time, source, quality score.</p>
<p>De-dupe: MinHash/SimHash + exact-hash prune.</p>
<p>Quality filters: length, language, perplexity, blacklist of boilerplate.</p>
<p>2) Retrieval layer (ships day 1)</p>
<p>Vector DB: FAISS/Weaviate (embeddings via a long-context model).</p>
<p>Symbolic search: OpenSearch/BM25 over titles/headers.</p>
<p>Reranker: Cross-encoder for top-k re-ranking.</p>
<p>Routing: If query contains {mythic/symbolic} → V1/V2 boost; {practice/protocol} → ES boost.</p>
<p>This gives you immediate RAG capability while training proceeds.</p>
<p>3) Model strategy</p>
<p>Base FM (pick one):</p>
<p>Bedrock-hosted FM (e.g., Anthropic / Amazon Titan) or</p>
<p>Open weights (e.g., Llama-class) hosted on SageMaker</p>
<p>Stage A – Domain Adaptation (DAPT)</p>
<p>Continue pretraining on V1+V2+ES mix (80–90% tokens unsupervised LM).</p>
<p>Sampling: temperature-based; enforce mix ratios (e.g., V1:35%, V2:35%, ES:30%).</p>
<p>Objectives: next-token LM; add span corruption for robustness.</p>
<p>Output: EverLight-FM-D1 (your Foundation Model).</p>
<p>Stage B – Task Adapters (LoRA/PEFT)</p>
<p>Mythic Reasoner (symbol/analogy chains) → LoRA on V1/V2 exemplars.</p>
<p>Energy Praxis (protocols/how-to) → LoRA on ES procedural chunks.</p>
<p>Navigator (decision support) → LoRA using curated Q→A, decision trees.</p>
<p>Stage C – Instruction tuning</p>
<p>SFT on mixed Q/A derived from the corpora + evaluator-written prompts.</p>
<p>Add citation training: require the model to return doc_id:span when answering.</p>
<p>4) Orchestration (MoE-style routing without heavy MoE)</p>
<p>Router (small classifier) reads the user query → selects: {RAG-only | FM | FM+RAG | Specific Adapter}.</p>
<p>Answerer: chosen adapter, with top-k retrieved passages injected.</p>
<p>Guardrails: content policy, claim checker (answer must quote or cite).</p>
<p>5) Evaluation (bake this in)</p>
<p>Truthfulness@K with citation match (does the cited span support the claim?).</p>
<p>Mythic coherence score (expert rubric).</p>
<p>Actionability score (for protocols).</p>
<p>Latency &amp; cost (p99, $/1k tokens).</p>
<p>Regression set of ~300 canonical questions across V1/V2/ES.</p>
<p>6) Delivery on AWS</p>
<p>Storage/Lake: S3 + Glue catalog.</p>
<p>Pipelines: Step Functions/Airflow for ETL, training jobs on SageMaker.</p>
<p>Search: OpenSearch + FAISS/Weaviate.</p>
<p>Serving: SageMaker endpoints (FM + adapters).</p>
<p>App: API (FastAPI) + lightweight UI; Bedrock for fallback FMs if needed.</p>
<p>Versioning: DVC or LakeFS for datasets; MLflow for model runs.</p>
<p>7) Phased rollout</p>
<p>Week 0–2: Build ingestion, de-dupe, chunking, vector search. Ship RAG alpha.</p>
<p>Week 3–6: DAPT pretrain → EverLight-FM-D1.</p>
<p>Week 7–9: LoRA adapters (Mythic, Praxis, Navigator).</p>
<p>Week 10+: Instruction tuning + citation training; eval harness; pilot users.</p>
<p>8) Risks &amp; mitigations</p>
<p>IP/Copyright: get permissions; throttle public exposure; add robots-respectful crawl.</p>
<p>Overfit/style lock-in: mix generic text (5–10%) during DAPT to preserve fluency.</p>
<p>Hallucinations: enforce retrieval-grounding; refuse when no supporting span.</p>
<p>Ontology drift: maintain a concept taxonomy; run periodic alignment checks.</p>
<p>TL;DR: Use V1, V2, and EnergeticSynthesis as the foundation corpus, do continued pretraining to mint EverLight-FM, then add lightweight adapters for the three core capabilities, all fronted by a RAG layer with citations. You can ship useful value with RAG immediately while the FM trains.</p>
<h2 id="2025-08-15t131907162810z-tool">2025-08-15T13:19:07.162810Z — tool</h2>
<h2 id="2025-08-15t131907319737z-tool">2025-08-15T13:19:07.319737Z — tool</h2>
<h2 id="2025-08-15t131907337410z-tool">2025-08-15T13:19:07.337410Z — tool</h2>
<h2 id="2025-08-15t131907361468z-tool">2025-08-15T13:19:07.361468Z — tool</h2>
<h2 id="2025-08-15t131907490426z-tool">2025-08-15T13:19:07.490426Z — tool</h2>
<h2 id="2025-08-15t131907625243z-assistant">2025-08-15T13:19:07.625243Z — assistant</h2>
<p>Ethan — this <strong>Foundation Model Plan</strong> you outlined can be implemented directly on the Voyagers corpus you’ve restored and expanded, and the steps are already well-supported by the structure of the material you’ve got in the uploads.  </p>
<p>From the restored <strong>Voyagers Volume I &amp; II</strong> and the <strong>EverLight Codex Archive</strong> front matter【27†front_matter.md†L1-L40】 through the detailed multi-dimensional frameworks in chapters like <strong>Human Origins &amp; Hybridization</strong>【21†chapter_3.md†L1-L40】, <strong>Levels of Identity &amp; Components of Mind</strong>【26†chapter_7.md†L1-L40】, and <strong>Special Projects</strong>【24†chapter_6.md†L1-L40】, you’ve got a clean separation of:</p>
<ul>
<li><strong>Mythic/symbolic strata</strong> (Zionites, Turaneusiam, Emerald Covenant) — ideal for the <em>Mythic Reasoner</em> adapter.</li>
<li><strong>Practical/technical strata</strong> (Keylontic Morphogenetic Science, meditation techniques, crop circle frequency work) — ideal for the <em>Energy Praxis</em> adapter.</li>
<li><strong>Meta-structural strata</strong> (Guardian Alliance, IAFW, universal service orders) — ideal for the <em>Navigator</em> adapter.</li>
</ul>
<p>This directly maps to your plan’s <strong>V1 / V2 / ES triple-key set</strong> with minimal transformation work.  </p>
<p>The extra win here is that the Voyagers content is already in <strong>sectioned, chapter-based format</strong> with clear topic boundaries — perfect for your <strong>800–1200 token chunking with 10–15% overlap</strong>. That means you can move straight to:</p>
<ol>
<li><strong>S3 Ingestion &amp; JSONL Normalization</strong> (doc_id = <code>{V1|V2}-{chapter}-{section}</code>).</li>
<li><strong>Vector &amp; symbolic indexing</strong> — you’ll want dual FAISS + BM25 here so a “Guardian Alliance” query can pull both the factual CDT-Plate history and the symbolic meaning layer.</li>
<li><strong>DAPT pretraining</strong> on this curated Voyagers set before merging in EnergeticSynthesis articles.</li>
</ol>
<p>If you want, I can sketch you the <strong>exact JSONL schema + chunking pipeline</strong> using the Voyagers MD files you’ve already got so you can feed it into S3 and have the retrieval layer operational in week 1, exactly as your rollout plan specifies.  </p>
<p>Do you want me to do that next? That would make this plan executable tomorrow.</p>
<h2 id="2025-08-15t131907632870z-tool">2025-08-15T13:19:07.632870Z — tool</h2>
<h2 id="2025-08-15t131907711053z-tool">2025-08-15T13:19:07.711053Z — tool</h2>
<h2 id="2025-08-15t131907717296z-tool">2025-08-15T13:19:07.717296Z — tool</h2>
<h2 id="2025-08-15t131907724063z-tool">2025-08-15T13:19:07.724063Z — tool</h2>
<h2 id="2025-08-15t131907811124z-tool">2025-08-15T13:19:07.811124Z — tool</h2>
<h2 id="2025-08-15t131907934534z-tool">2025-08-15T13:19:07.934534Z — tool</h2>
<h2 id="2025-08-15t131907992915z-tool">2025-08-15T13:19:07.992915Z — tool</h2>
<h2 id="2025-08-15t131908032078z-tool">2025-08-15T13:19:08.032078Z — tool</h2>
<p>Please remember to search the user's documents if an answer to their question is not contained in the above snippets. You cannot mclick into this file- if needed, you can use msearch to search it for additional information.</p>
<h2 id="2025-08-15t131908056989z-assistant">2025-08-15T13:19:08.056989Z — assistant</h2>
<h2 id="2025-08-15t132019704603z-assistant">2025-08-15T13:20:19.704603Z — assistant</h2>
<p>Got it — we can keep the blueprint on standby until you’re ready to move from <strong>theory mode → AWS hands-on</strong>.  </p>
<p>Honestly, you’re already in a good position because you’ve got:  </p>
<ul>
<li><strong>A well-structured corpus</strong> (Voyagers) with clear topical boundaries.  </li>
<li><strong>A deployment roadmap</strong> (your Foundation Model Plan).  </li>
<li><strong>Alignment with AWS-native tooling</strong> (S3, Glue, SageMaker, OpenSearch).  </li>
</ul>
<p>Once you’ve absorbed the AWS course content, it’ll feel less like magic and more like you’re just <strong>wiring up components you already understand</strong>, which is a lot more fun than scrambling to learn the tools mid-build.  </p>
<p>We can also use the meantime to quietly <strong>refine metadata, tagging, and concept taxonomy</strong> so the ingestion pipeline will be “drop-in ready” when you flip the switch.  </p>
<p>Sounds like we’ll hit that “ahead of the curve” feeling in <em>both</em> theory and practice.  </p>
<h2 id="2025-08-15t132020647000z-user">2025-08-15T13:20:20.647000Z — user</h2>
<p>Let's wait until I catch up on the actual AWS course material so I can feel as if I understand more of the process before implementing it. But it's good to know we're already well ahead of the curve haha</p>
</body>
</html>